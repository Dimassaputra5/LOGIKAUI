{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8a7b00d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T16:48:52.961077Z",
     "iopub.status.busy": "2025-09-24T16:48:52.960332Z",
     "iopub.status.idle": "2025-09-24T16:48:58.712885Z",
     "shell.execute_reply": "2025-09-24T16:48:58.712020Z",
     "shell.execute_reply.started": "2025-09-24T16:48:52.961053Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import models\n",
    "# import torch._dynamo\n",
    "# torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor, Callback\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from torchmetrics.classification import MulticlassAccuracy, F1Score\n",
    "# !uv pip install pytorch_optimizer\n",
    "import pytorch_optimizer as optim1\n",
    "\n",
    "pl.seed_everything(42)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63f65c46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T16:48:58.715054Z",
     "iopub.status.busy": "2025-09-24T16:48:58.714413Z",
     "iopub.status.idle": "2025-09-24T16:48:58.722284Z",
     "shell.execute_reply": "2025-09-24T16:48:58.721586Z",
     "shell.execute_reply.started": "2025-09-24T16:48:58.715030Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Path ke direktori berisi semua gambar test.\n",
    "            transform (callable, optional): Transformasi yang akan diterapkan pada gambar.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        allowed_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.gif'}\n",
    "        self.image_files = sorted([\n",
    "        f for f in os.listdir(root_dir) if os.path.isfile(os.path.join(root_dir, f)) and os.path.splitext(f)[1].lower() in allowed_extensions\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Mengembalikan jumlah total gambar dalam dataset.\"\"\"\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Mengambil satu item data.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Indeks dari item.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (image, image_name) di mana image_name adalah nama file.\n",
    "        \"\"\"\n",
    "        img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        image_name = self.image_files[idx]\n",
    "        return image, image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8be3aa77-d154-4a0c-a364-693526f20b23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T16:48:58.723647Z",
     "iopub.status.busy": "2025-09-24T16:48:58.723091Z",
     "iopub.status.idle": "2025-09-24T16:48:58.745385Z",
     "shell.execute_reply": "2025-09-24T16:48:58.744742Z",
     "shell.execute_reply.started": "2025-09-24T16:48:58.723624Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageClassification(\n",
      "    crop_size=[224]\n",
      "    resize_size=[256]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BICUBIC\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "weights = models.EfficientNet_B0_Weights.IMAGENET1K_V1\n",
    "auto_transforms = weights.transforms()\n",
    "print(auto_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e664d77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T16:48:58.747128Z",
     "iopub.status.busy": "2025-09-24T16:48:58.746865Z",
     "iopub.status.idle": "2025-09-24T16:48:59.544597Z",
     "shell.execute_reply": "2025-09-24T16:48:59.543921Z",
     "shell.execute_reply.started": "2025-09-24T16:48:58.747087Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pemetaan kelas: {'balinese': 0, 'batak': 1, 'dayak': 2, 'javanese': 3, 'minangkabau': 4}\n",
      "Jumlah sampel per kelas: tensor([776,  95,  69, 249, 563])\n",
      "Bobot untuk setiap kelas: tensor([0.0013, 0.0105, 0.0145, 0.0040, 0.0018])\n",
      "Panjang bobot per sampel: 1752\n",
      "Contoh 5 bobot pertama: tensor([0.0013, 0.0013, 0.0013, 0.0013, 0.0013])\n",
      "Jumlah data test: 1752\n"
     ]
    }
   ],
   "source": [
    "train= transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.RandomPerspective(),\n",
    "            transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),\n",
    "            transforms.ColorJitter(),\n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            auto_transforms\n",
    "])\n",
    "val = transforms.Compose([\n",
    "        auto_transforms\n",
    "])\n",
    "data_train = 'dsc-logika-ui-2025\\Train\\Train'\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=data_train, transform=train)\n",
    "labels = train_dataset.targets\n",
    "class_counts = torch.bincount(torch.tensor(labels))\n",
    "print(f\"Pemetaan kelas: {train_dataset.class_to_idx}\")\n",
    "print(f\"Jumlah sampel per kelas: {class_counts}\")\n",
    "class_weights = 1.0 / class_counts.float()\n",
    "print(f\"Bobot untuk setiap kelas: {class_weights}\")\n",
    "weights_per_sample = class_weights[labels]\n",
    "print(f\"Panjang bobot per sampel: {len(weights_per_sample)}\")\n",
    "print(\"Contoh 5 bobot pertama:\", weights_per_sample[:5])\n",
    "\n",
    "sampler = WeightedRandomSampler(weights_per_sample, num_samples=len(weights_per_sample), replacement=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True, sampler=sampler)\n",
    "print(f'Jumlah data test: {len(train_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ab43021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataset = datasets.ImageFolder(root='dataset_split/validation', transform=val)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "# print(f'Jumlah data test: {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b58622c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T16:48:59.545543Z",
     "iopub.status.busy": "2025-09-24T16:48:59.545291Z",
     "iopub.status.idle": "2025-09-24T16:48:59.758182Z",
     "shell.execute_reply": "2025-09-24T16:48:59.757549Z",
     "shell.execute_reply.started": "2025-09-24T16:48:59.545525Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah data test: 444\n"
     ]
    }
   ],
   "source": [
    "data_test_dir = 'dsc-logika-ui-2025\\Test\\Test'\n",
    "\n",
    "test_dataset = TestDataset(root_dir=data_test_dir, transform=val)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "print(f'Jumlah data test: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b9a122c5-e7b2-42ed-aed7-9a77b1230c57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T16:48:59.759131Z",
     "iopub.status.busy": "2025-09-24T16:48:59.758902Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# feature, target = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e5ebf143",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# feature.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79885754",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'balinese': 0, 'batak': 1, 'dayak': 2, 'javanese': 3, 'minangkabau': 4}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2cat, idxclass = train_dataset.class_to_idx, train_dataset.classes\n",
    "label2cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33863dd",
   "metadata": {},
   "source": [
    "## Arsitektur dan config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "679c7a9f",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def conv_block(in_feature, out_feature, padding=1, stride=1,\n",
    "             activation=\"relu\", pool =True, maxpool=True, kernel_size=3,\n",
    "             kernel_size_pool=2, pool_stride=2)-> list[nn.Sequential]:\n",
    "    layers = [nn.Conv2d(in_feature, out_feature, kernel_size=kernel_size, padding=padding, stride=stride)]\n",
    "    if activation == \"relu\":\n",
    "        layers.append(nn.ReLU())\n",
    "    elif activation == \"leakyrelu\":\n",
    "        layers.append(nn.LeakyReLU())\n",
    "    elif activation == \"sigmoid\":\n",
    "        layers.append(nn.Sigmoid())\n",
    "    elif activation == 'mish': layers.append(nn.Mish())\n",
    "    elif activation == \"tanh\":\n",
    "        layers.append(nn.Tanh())\n",
    "    if pool:\n",
    "        if maxpool:\n",
    "            layers.append(nn.MaxPool2d(kernel_size=kernel_size_pool, stride=pool_stride))\n",
    "        else:\n",
    "            layers.append(nn.AvgPool2d(kernel_size=kernel_size_pool, stride=pool_stride))\n",
    "    else:\n",
    "        layers.append(nn.Identity())\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def linear_block(in_features, out_features, activation=None, dropout=0.0, batch_norm=None):\n",
    "    layers = [nn.Linear(in_features, out_features)]\n",
    "    if batch_norm:\n",
    "        layers.append(nn.BatchNorm1d(out_features))\n",
    "    if activation == 'relu':\n",
    "        layers.append(nn.ReLU())\n",
    "    elif activation == 'sigmoid':\n",
    "        layers.append(nn.Sigmoid())\n",
    "    elif activation == 'tanh':\n",
    "        layers.append(nn.Tanh())\n",
    "    elif activation == 'leakyrelu':\n",
    "        layers.append(nn.LeakyReLU())\n",
    "    elif activation == 'mish': layers.append(nn.Mish())\n",
    "    elif activation == 'softmax':\n",
    "        layers.append(nn.Softmax(dim=1))\n",
    "    elif activation == 'elu':\n",
    "        layers.append(nn.ELU())\n",
    "    elif activation == 'selu':\n",
    "        layers.append(nn.SELU())\n",
    "    elif activation == 'lsoftmax':\n",
    "        layers.append(nn.LogSoftmax(dim=1))\n",
    "    if dropout > 0.0:\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "27a449c0",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self, dropout=0.0, freeze=True):\n",
    "        super().__init__()\n",
    "        weights = models.EfficientNet_B0_Weights.IMAGENET1K_V1\n",
    "        self.backbone = models.efficientnet_b0(weights=weights).features\n",
    "        if freeze:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.classifier = nn.Sequential(\n",
    "            linear_block(1280, 256, activation='mish', dropout=dropout, batch_norm=True),\n",
    "            linear_block(256, 128, activation='mish', dropout=dropout, batch_norm=True),\n",
    "            linear_block(128, 5, activation=None)\n",
    "        )\n",
    "    def forward(self, X):\n",
    "        X = self.backbone(X)\n",
    "        X = X.mean([2, 3])  \n",
    "        return self.classifier(X)\n",
    "        \n",
    "class PL(LightningModule):\n",
    "    def __init__(self, model, learning_rate, class_weights) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        self.macroF1 = F1Score(num_classes=5, average='macro', task='multiclass')\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.model(X)\n",
    "    \n",
    "    def _common_step(self, batch, batch_idx):\n",
    "        X, labels = batch\n",
    "        outputs = self(X) \n",
    "        loss = self.criterion(outputs, labels)\n",
    "        macrof1 = self.macroF1(outputs, labels)\n",
    "        return loss, macrof1\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, macroF1 = self._common_step(batch, batch_idx)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('train_macrof1', macroF1, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, macroF1 = self._common_step(batch, batch_idx)\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_macrof1', macroF1, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, macroF1 = self._common_step(batch, batch_idx)\n",
    "        self.log('test_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('test_macrof1', macroF1, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "    # def backward(self, loss, *args, **kwargs):\n",
    "    #     loss.backward(create_graph=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim1.Lion(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        \"\"\"\n",
    "        Langkah prediksi untuk satu batch data test.\n",
    "        \"\"\"\n",
    "        images, image_names = batch\n",
    "        outputs = self.forward(images)\n",
    "        _, predicted_labels = torch.max(outputs, 1)\n",
    "        return {\"image_names\": image_names, \"preds\": predicted_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2564e101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "class FineTuningCallback(Callback):\n",
    "    def __init__(self, unfreeze_at_epoch=8, backbone_lr=1e-5, head_lr=1e-4):\n",
    "        super().__init__()\n",
    "        self.unfreeze_at_epoch = unfreeze_at_epoch\n",
    "        self.backbone_lr = backbone_lr\n",
    "        self.head_lr = head_lr\n",
    "\n",
    "    def on_train_epoch_start(self, trainer, pl_module):\n",
    "        if trainer.current_epoch != self.unfreeze_at_epoch:\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n--- Epoch {self.unfreeze_at_epoch}: Fine-tuning diaktifkan! ---\")\n",
    "        \n",
    "        # 1. Cairkan (unfreeze) semua parameter di model\n",
    "        for param in pl_module.model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # 2. Dapatkan optimizer saat ini\n",
    "        optimizer = trainer.optimizers[0]\n",
    "        \n",
    "        # --- PERBAIKAN DIMULAI DI SINI ---\n",
    "        \n",
    "        # 3. Ambil hyperparameter default dari optimizer (termasuk 'betas')\n",
    "        defaults = optimizer.defaults\n",
    "        \n",
    "        # 4. Buat param_groups baru dengan menyertakan defaults tersebut,\n",
    "        #    lalu timpa 'params' dan 'lr' sesuai kebutuhan.\n",
    "        param_groups = [\n",
    "            # Grup untuk backbone: salin defaults, lalu timpa lr dan params\n",
    "            {**defaults, \"params\": pl_module.model.backbone.parameters(), \"lr\": self.backbone_lr},\n",
    "            \n",
    "            # Grup untuk classifier head\n",
    "            {**defaults, \"params\": pl_module.model.classifier.parameters(), \"lr\": self.head_lr}\n",
    "        ]\n",
    "        \n",
    "        # --- AKHIR DARI PERBAIKAN ---\n",
    "        \n",
    "        # 5. Hapus param_groups yang lama dan ganti dengan yang baru\n",
    "        optimizer.param_groups = param_groups\n",
    "        \n",
    "        print(f\"Optimizer dikonfigurasi ulang dengan LR backbone={self.backbone_lr} dan LR head={self.head_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19ac7d8",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    accelerator_type = 'gpu'\n",
    "    devices_to_use = 1\n",
    "else:\n",
    "    accelerator_type = 'cpu'\n",
    "    devices_to_use = 'auto'\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='train_macrof1',\n",
    "    dirpath='checkpoints/',\n",
    "    filename='logikaui-{epoch:02d}-{train_macrof1:.2f}',\n",
    "    save_top_k=1,\n",
    "    mode='max'\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='train_loss',\n",
    "    patience=10,\n",
    "    mode='min',\n",
    ")\n",
    "lr_monitor_callback = LearningRateMonitor(logging_interval='epoch')\n",
    "fine_tune_callback = FineTuningCallback(unfreeze_at_epoch=8, backbone_lr=1e-5, head_lr=3e-4)\n",
    "\n",
    "trainer1 = pl.Trainer(\n",
    "    max_epochs=300,\n",
    "    callbacks=[checkpoint_callback, early_stopping, lr_monitor_callback, fine_tune_callback],\n",
    "    logger=TensorBoardLogger(\"tb_logs\", name=\"simple_model_experiment\"),\n",
    "    accelerator=accelerator_type,\n",
    "    devices=devices_to_use,\n",
    "    log_every_n_steps=10,\n",
    "    deterministic=True,\n",
    "    precision='16-mixed'\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad56595",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c3324805-104f-4284-8da7-7cc954a207bd",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0013, 0.0105, 0.0145, 0.0040, 0.0018])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9a5c98a3",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = PL(EfficientNet(dropout=0.1, freeze=True), learning_rate=5e-4, class_weights=class_weights)\n",
    "# model = torch.compile(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6d142303",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type              | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | model     | EfficientNet      | 4.4 M  | train\n",
      "1 | criterion | CrossEntropyLoss  | 0      | train\n",
      "2 | macroF1   | MulticlassF1Score | 0      | train\n",
      "--------------------------------------------------------\n",
      "362 K     Trainable params\n",
      "4.0 M     Non-trainable params\n",
      "4.4 M     Total params\n",
      "17.479    Total estimated model params size (MB)\n",
      "348       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3462610954dd42d39981fa3ac8a2c9b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 5: Fine-tuning diaktifkan! ---\n",
      "Optimizer dikonfigurasi ulang dengan LR backbone=1e-05 dan LR head=0.0001\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'betas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m trainer1.fit(model, train_loader, val_dataloaders=\u001b[38;5;28;01mNone\u001b[39;00m, ckpt_path=\u001b[33m'\u001b[39m\u001b[33mlast\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m call._call_and_handle_interrupt(\n\u001b[32m    562\u001b[39m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[32m    563\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:49\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     48\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(*args, **kwargs)\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     52\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    592\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28mself\u001b[39m._run(model, ckpt_path=ckpt_path)\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    602\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28mself\u001b[39m._run_stage()\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1017\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1056\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1054\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_sanity_check()\n\u001b[32m   1055\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m         \u001b[38;5;28mself\u001b[39m.fit_loop.run()\n\u001b[32m   1057\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1058\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.state\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:216\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    215\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28mself\u001b[39m.advance()\n\u001b[32m    217\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end()\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:455\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trainer.profiler.profile(\u001b[33m\"\u001b[39m\u001b[33mrun_training_epoch\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28mself\u001b[39m.epoch_loop.run(\u001b[38;5;28mself\u001b[39m._data_fetcher)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:152\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done:\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m         \u001b[38;5;28mself\u001b[39m.advance(data_fetcher)\n\u001b[32m    153\u001b[39m         \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:348\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    345\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33m\"\u001b[39m\u001b[33mrun_training_batch\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    346\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.lightning_module.automatic_optimization:\n\u001b[32m    347\u001b[39m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m         batch_output = \u001b[38;5;28mself\u001b[39m.automatic_optimization.run(trainer.optimizers[\u001b[32m0\u001b[39m], batch_idx, kwargs)\n\u001b[32m    349\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    350\u001b[39m         batch_output = \u001b[38;5;28mself\u001b[39m.manual_optimization.run(kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:192\u001b[39m, in \u001b[36m_AutomaticOptimization.run\u001b[39m\u001b[34m(self, optimizer, batch_idx, kwargs)\u001b[39m\n\u001b[32m    185\u001b[39m         closure()\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28mself\u001b[39m._optimizer_step(batch_idx, closure)\n\u001b[32m    194\u001b[39m result = closure.consume_result()\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:270\u001b[39m, in \u001b[36m_AutomaticOptimization._optimizer_step\u001b[39m\u001b[34m(self, batch_idx, train_step_and_backward_closure)\u001b[39m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28mself\u001b[39m.optim_progress.optimizer.step.increment_ready()\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m call._call_lightning_module_hook(\n\u001b[32m    271\u001b[39m     trainer,\n\u001b[32m    272\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moptimizer_step\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    273\u001b[39m     trainer.current_epoch,\n\u001b[32m    274\u001b[39m     batch_idx,\n\u001b[32m    275\u001b[39m     optimizer,\n\u001b[32m    276\u001b[39m     train_step_and_backward_closure,\n\u001b[32m    277\u001b[39m )\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[32m    280\u001b[39m     \u001b[38;5;28mself\u001b[39m.optim_progress.optimizer.step.increment_completed()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:177\u001b[39m, in \u001b[36m_call_lightning_module_hook\u001b[39m\u001b[34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m pl_module._current_fx_name = hook_name\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     output = fn(*args, **kwargs)\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    180\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\core\\module.py:1366\u001b[39m, in \u001b[36mLightningModule.optimizer_step\u001b[39m\u001b[34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[39m\n\u001b[32m   1335\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimizer_step\u001b[39m(\n\u001b[32m   1336\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1337\u001b[39m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1340\u001b[39m     optimizer_closure: Optional[Callable[[], Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1341\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1342\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[32m   1343\u001b[39m \u001b[33;03m    the optimizer.\u001b[39;00m\n\u001b[32m   1344\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1364\u001b[39m \n\u001b[32m   1365\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1366\u001b[39m     optimizer.step(closure=optimizer_closure)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:154\u001b[39m, in \u001b[36mLightningOptimizer.step\u001b[39m\u001b[34m(self, closure, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[33m\"\u001b[39m\u001b[33mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m step_output = \u001b[38;5;28mself\u001b[39m._strategy.optimizer_step(\u001b[38;5;28mself\u001b[39m._optimizer, closure, **kwargs)\n\u001b[32m    156\u001b[39m \u001b[38;5;28mself\u001b[39m._on_after_step()\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:239\u001b[39m, in \u001b[36mStrategy.optimizer_step\u001b[39m\u001b[34m(self, optimizer, closure, model, **kwargs)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl.LightningModule)\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\plugins\\precision\\amp.py:76\u001b[39m, in \u001b[36mMixedPrecision.optimizer_step\u001b[39m\u001b[34m(self, optimizer, model, closure, **kwargs)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimizer_step\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     72\u001b[39m     **kwargs: Any,\n\u001b[32m     73\u001b[39m ) -> Any:\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     75\u001b[39m         \u001b[38;5;66;03m# skip scaler logic, as bfloat16 does not require scaler\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(optimizer, LBFGS):\n\u001b[32m     78\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[33m\"\u001b[39m\u001b[33mAMP and the LBFGS optimizer are not compatible.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision.py:123\u001b[39m, in \u001b[36mPrecision.optimizer_step\u001b[39m\u001b[34m(self, optimizer, model, closure, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[32m    122\u001b[39m closure = partial(\u001b[38;5;28mself\u001b[39m._wrap_closure, model, optimizer, closure)\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m optimizer.step(closure=closure, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    482\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    483\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    484\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    485\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m out = func(*args, **kwargs)\n\u001b[32m    488\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dsapu\\Project\\conda\\condasystem\\envs\\dmsdl\\Lib\\site-packages\\pytorch_optimizer\\optimizer\\lion.py:83\u001b[39m, in \u001b[36mLion.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     81\u001b[39m     group[\u001b[33m'\u001b[39m\u001b[33mstep\u001b[39m\u001b[33m'\u001b[39m] += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m beta1, beta2 = group[\u001b[33m'\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m group[\u001b[33m'\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m     86\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m p.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyError\u001b[39m: 'betas'"
     ]
    }
   ],
   "source": [
    "trainer1.fit(model, train_loader, val_dataloaders=None, ckpt_path='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738ba917",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    preds = trainer1.predict(model, test_loader, ckpt_path='best')\n",
    "    print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f001afb",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for batch_result in preds:\n",
    "    # batch_result adalah dictionary yang kita return dari predict_step\n",
    "    image_names = batch_result['image_names']\n",
    "    preds = batch_result['preds'].cpu().numpy() # Pindahkan ke CPU\n",
    "    \n",
    "    for name, label in zip(image_names, preds):\n",
    "        predictions.append({\n",
    "            'id': name,      # Nama kolom sesuai standar Kaggle\n",
    "            'style': label   # Nama kolom sesuai standar Kaggle\n",
    "        })\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da01ac3-565b-4a53-90f7-6730ac16ea1a",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9f8d91-ee0e-4b98-833c-0615ed20cc7d",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class_mapping = {'balinese': 0, 'batak': 1, 'dayak': 2, 'javanese': 3, 'minangkabau': 4}\n",
    "idx_to_class = {v: k for k, v in class_mapping.items()}\n",
    "# Cek tipe data dari kolom 'style'. Kemungkinan besar hasilnya 'object' (string).\n",
    "print(f\"Tipe data kolom 'style': {submission_df['style'].dtype}\")\n",
    "submission_df['id'] = submission_df['id'].str.split('.').str[0]\n",
    "\n",
    "# Lihat nilai-nilai unik di dalam kolom tersebut.\n",
    "# Perhatikan apakah ada tanda kutip, yang menandakan string.\n",
    "print(f\"Nilai unik di kolom 'style': {submission_df['style'].unique()}\")\n",
    "submission_df['style'] = submission_df['style'].map(idx_to_class)\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ced9ca8-ef3c-4aa6-b915-1e4272a0c6fd",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a07a0a-b589-4fb8-b9f3-f78788195eac",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 13809662,
     "sourceId": 115446,
     "sourceType": "competition"
    },
    {
     "datasetId": 8336088,
     "sourceId": 13156543,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "dmsdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
